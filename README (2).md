
# Gradient Descent for Optimization

Gradient descent is an algorithm used to the find local minima of any differentiable function. More formally, given a differentiable function  f(x),  the gradient descent algorithms helps us compute  x∗  such that  f′(x∗)=0  and  x∗  is a minimum of  f(x).  A function can have many local minima  x∗1,x∗2,…,x∗k,  the gradient descent algorithm will converge to one of them depending on the its starting position and learning rate (discussed below).
The name "Gradient Descent" hints at how this algorithm works. The algorithms requires 1) knowing the  gradient  (partial derivatives) of the function and 2) using the gradient to determine the direction of steepest  descent.  The idea is that we will begin somewhere on the function, for example  f(x1) , and then climb down the function as quickly as possible towards the first valley (local minimum) where  f′(x∗)=0.  The gradient descent algorithm essentially describes the sequence of steps to take to go from  x1  to a local minimum  x∗. 
Gradient descent (or a variation) is very commonly used to train machine learning models by minimizing some objective function. Many of the objective functions in machine learning are hard to minimize analytically, but we can approximate the minimum using gradient descent.


## Algorithm intuition
Suppose we have a single variable function  f(x)  that has a single global minimum (for example a parabola that opens upwards) at  x∗.  Our goal is to approximate  x∗  through an iterative algorithm. The first step is to pick a starting point (initial value) for the algorithm,  x1.  We will randomly guess  x1  and note that either  x1<x∗ , or  x1>x∗  (we can get really lucky and have  x1=x∗,  but this is very unlikely). Since  x∗  is the global minimum, we know that  f(x∗)<f(x1),  that is our starting point is above the minimum. Starting at  x1  we want to take a sequence of steps to get down to  x∗.  The gradient descent algorithm characterizes the set of steps to take to get from  x1  down to  x∗. 
Without loss of generality assume our starting position  x1<x∗,  and let us discuss how to get down towards  x∗.  Recall that we can compute the gradient of  f(x)  since we assume the function is differentiable. Suppose we find  f′(x1)<0,  that indicates that if we take a step right to  x2>x1  then we will move down the function since  f(x2)<f(x1).  This is exactly what we want (moving down the function), so we want to take a step to the right of  x1.  But how large of a step should we take? Well it makes sense to say the step size will depend on the steepness of the descent. The steeper the descent at  x1 , the larger the step size should be as it indicates we have a long way to go before getting to the minimum. More formally the step size will be proportional to the gradient at  x1. 
Once we get from  x1  to  x2 , we repeat the same logic as above. Suppose  f′(x2)<0  again, so we need to take a step towards  x3>x2  to further go down the function. Again our step size will be proportional to  f′(x2).  Repeating this process will result in a sequence of steps  x1,x2,…,xT.  For large values of  T  we expect  xT≈x∗.  The diagram below more formally describes the intuition behind gradient descent.
To determine  x2  we take a step to the right of  x1  that is proportional to  f′(x1).  Mathematically speaking, the sign of  f′(x1)  indicates the direction of the steepest ascent at  x1  (magnitude of  f′(x1)  is measure of steepness), but since we want to descend (its called gradient  descent ) we use  −f′(x1).  Gradient descent relies on a parameter  λ  called the learning rate, let us talk about this more.
    


## Thinking about the learning rate
The learning rate  λ  and the steepest descent  −f′(x1)  together determine the step size towards the minimum. Hence the learning rate determines the size of the steps. If  λ  is large then we will take large steps down towards to the minimum. Similarly if  λ  is small it will take longer to converge towards the minimum as the step size is smaller. Does this mean we should pick a really large  λ  to coverge fast towards  x∗?  No, this is not a good idea.  λ  too large or too small can cause covergence problems as illustrated by the diagram below.
Note that  J(w)  is the function that we want to minimize in the above diagram. On the left figure  λ  is very large which leads to taking very big steps. These big steps essentially have trouble locating the minimum because they are jumping over it. On the right figure  λ  is very small leading to tiny steps towards the minimum. The algorithm finds a minimum, but its only a local minimum (global minimum is ideal). For the right figure imagine the step size was a bit larger, then the algorithm could jump over the local minimum and possiblly converge into the global minimum.
So how you do pick the best  λ ? This is often done using "cross validation" which will be discussed later on.
## Gradient descent algorithm
Single variable
Suppose we have a single variable and differentiable function  f(x).  Given a initial value  x1  the gradient descent describes the steps required to coverge towards a local minimum  x∗  such that  f′(x∗)=0.  In the above section we determined that  x2=x1−λf′(x1),  where  λ  is the learning rate parameter. This formula can be generalized to
xt+1=xt−λf′(xt), 
for iteration  t.  Given initial value  x1  and large number of iterations  T,  this algorithm will generate  x1,x2,…,xT,  where  xT≈x∗.  That is  xt+1  converges towards  x∗  as  t  gets very large. Notice that at convergence  f′(xt)≈0  and hence  |xt+1−xt|≈0.  Therefore a common stopping criteria for gradient descent is to iterate until  |xt+1−xt|  is a very small number. For example we can keep iterating gradient descent until  |xt+1−xt|<0.001. 
Multiple variables
Consider a multiple variable and differentiable function  f(x1,x2,…,xn).  Our goal is to apply gradient descent and find a minimum  (x∗1,x∗2,…,x∗n).  This function has partial derivatives stored in the gradient vector  (df(x)dx1,…,df(x)dxn).  The direction of the gradient vector indicates direction of steepest ascent, and the length of the gradient vector is a mesuare of the steepness. We can easily generalize the gradient descent to multiple variables as follows:
⎡⎣⎢⎢xt+11⋮xt+1n⎤⎦⎥⎥=⎡⎣⎢⎢xt1⋮xtn⎤⎦⎥⎥−λ⎡⎣⎢⎢⎢⎢df(x1)dx1,t⋮df(xn)dxn,t⎤⎦⎥⎥⎥⎥, 

where  t  is the gradient descent itteration. Now the stopping criteria can depend on the euclidean distance between the now and previous itteration, that is stop if  (xt+11−xt1)2+…+(xt+1n−xtn)2−−−−−−−−−−−−−−−−−−−−−−−−−√<0.001. 
Example: Minimizing single variable function
Consider the following single variable function
f(x)=0.1x2+sin(0.1x2)
and suppose we find its minimum using gradient descent. Gradient descent requires us to compute the first derivative which is
f′(x)=0.2x+0.2xcos(0.1x2).
The plot for f(x) is shown below.
## Conclusion
Gradient descent is a first order (requires first derivative) optimization algorithm used to minimize a given objective function. The algorithm depends on the initial value and the learning rate. The idea behind gradient descent is that it takes a seqeuence of steps to go from the starting position to a local minimum. An initial value that is somewhat close to the global minimum is ideal. Too small a learning rate makes it more likely for the algorithm to get stuck at a local minimum. Whereas too high a learning rate makes it more likely that the algorithm will skip over the global minimum. As we discussed in this notebook, the gradient descent can be applied to any first differentiable function (single or multiple variabe) to approximate the minimums (can be local minimums) even if analytical ( f′(x)=0  solve for x) solutions are difficult.